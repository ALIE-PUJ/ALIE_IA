{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install LlamaIndex Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install llama-index\\n!pip install llama-index-llms-gemini\\n!pip install llama-index-embeddings-huggingface\\n!pip install spacy\\n!pip install llama-index-llms-langchain\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install llama-index\n",
    "!pip install llama-index-llms-gemini\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install spacy\n",
    "!pip install llama-index-llms-langchain\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import os\n",
    "import time\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Definir el modelo local\n",
    "llm = ChatOpenAI(\n",
    "    model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\"    # organization=\"...\",\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# pip install langchain-groq\n",
    "# Definir el modelo groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"XXX\" # Definir la API Key de Groq\n",
    "api_key = os.getenv(\"GROQ_API_KEY\", \"NotFound\") # Obtener la API Key de las variables de entorno\n",
    "print(api_key)\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"18_EstructurasDeDatos_4196.json\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luis Alejandro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:200: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "c:\\Users\\Luis Alejandro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:296: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "eval_documents = documents[:3]\n",
    "data_generator = DatasetGenerator.from_documents(eval_documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the title and code of the subject related to data structures?', 'What is the purpose of studying data structures and algorithms in a real-world setting?', 'How many credits does this subject have in the pre-degree program?', 'What are the academic prerequisites for enrolling in this subject?', 'List the learning objectives of this subject.', 'What are the expected learning outcomes for students in this subject?', 'What are the four thematic contents of this subject?', 'Describe the first teaching strategy used in this subject.', 'Explain the fourth teaching strategy used in this subject and its benefits.', 'What are the two dates related to the creation and modification of the syllabus file?', 'What is the file type and size of the document \"18\\\\_EstructurasDeDatos\\\\_4196.json\"?', 'What are the three types of data structures covered in the course?', 'What is the weight of Parcial 1 in the overall course evaluation?', 'What is the objective of the first teaching strategy, \"aprendizaje directivo mediado mediante clases magistrales\"?', 'Which textbook by Weiss is recommended for this course? Please provide the edition and publication year.', 'What percentage is allocated to the \"Proyecto de desarrollo a lo largo del semestre\" in the overall course evaluation?', 'What is the objective of the fourth teaching strategy, \"aprendizaje colaborativo\"?', 'Which textbook by Nyhoff is recommended for this course? Please provide the edition and publication year.', 'What are the topics covered in Taller 1?', 'What is the objective of the second teaching strategy, \"aprendizaje basado en problemas mediante talleres individuales\"?']\n"
     ]
    }
   ],
   "source": [
    "print(eval_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up faithfulness and relevancy evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness = FaithfulnessEvaluator()\n",
    "relevancy = RelevancyEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(chunk_size, eval_questions):\n",
    "\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents\n",
    "    )\n",
    "\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "    # print(\"Number of questions: \", num_questions)\n",
    "\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        faithfulness_result = faithfulness.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "        \n",
    "        relevancy_result = relevancy.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using only 1 question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question to evaluate:  ['What is the purpose of studying data structures and algorithms in a real-world setting?']\n",
      "Chunk size 1 - Average Response time: 11.852120637893677s, Average Faithfulness: 1.0, Average Relevancy: 1.0\n",
      "Chunk size 2 - Average Response time: 27.78049921989441s, Average Faithfulness: 1.0, Average Relevancy: 1.0\n",
      "Chunk size 4 - Average Response time: 27.750017166137695s, Average Faithfulness: 1.0, Average Relevancy: 1.0\n",
      "Chunk size 8 - Average Response time: 28.74750590324402s, Average Faithfulness: 1.0, Average Relevancy: 1.0\n",
      "Chunk size 16 - Average Response time: 28.742502689361572s, Average Faithfulness: 1.0, Average Relevancy: 1.0\n",
      "Chunk size 32 - Average Response time: 28.722283363342285s, Average Faithfulness: 1.0, Average Relevancy: 1.0\n",
      "Chunk size 64 - Average Response time: 28.76851749420166s, Average Faithfulness: 0.0, Average Relevancy: 1.0\n",
      "Chunk size 128 - Average Response time: 27.713791131973267s, Average Faithfulness: 0.0, Average Relevancy: 1.0\n",
      "Chunk size 256 - Average Response time: 28.699707746505737s, Average Faithfulness: 0.0, Average Relevancy: 1.0\n",
      "Chunk size 512 - Average Response time: 28.67915630340576s, Average Faithfulness: 0.0, Average Relevancy: 1.0\n",
      "Chunk size 1024 - Average Response time: 28.741468906402588s, Average Faithfulness: 0.0, Average Relevancy: 1.0\n",
      "Chunk size 2048 - Average Response time: 27.72715425491333s, Average Faithfulness: 0.0, Average Relevancy: 1.0\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "\n",
    "one_question_array = []\n",
    "one_question_array.append(eval_questions[1])\n",
    "\n",
    "print(\"Question to evaluate: \", one_question_array)\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate(chunk_size, one_question_array)\n",
    "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time}s, Average Faithfulness: {avg_faithfulness}, Average Relevancy: {avg_relevancy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using 10 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sizes = [128, 256, 512, 1024, 2048]\n",
    "\n",
    "ten_question_array = []\n",
    "for i in range(10):  # Desde 0 hasta 9 inclusive\n",
    "    ten_question_array.append(eval_questions[i])\n",
    "\n",
    "print(\"Question to evaluate: \", ten_question_array)\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate(chunk_size, ten_question_array)\n",
    "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time}s, Average Faithfulness: {avg_faithfulness}, Average Relevancy: {avg_relevancy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sizes = [128, 256, 512, 1024, 2048]\n",
    "\n",
    "print(\"Questions to evaluate: \", eval_questions)\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate(chunk_size, eval_questions)\n",
    "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time}s, Average Faithfulness: {avg_faithfulness}, Average Relevancy: {avg_relevancy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
